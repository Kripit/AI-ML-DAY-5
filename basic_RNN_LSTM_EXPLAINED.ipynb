# Complete RNN & LSTM Guide: Basic to Advanced

## Essential Imports for RNN/LSTM

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import math
import time

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
torch.manual_seed(42)
```

## Core PyTorch Functions for RNN/LSTM

### Tensor Operations Essential for RNNs

```python
# ====== TENSOR CREATION & MANIPULATION ======

# torch.zeros() - Initialize hidden states
batch_size, hidden_size, seq_len = 32, 128, 10
h0 = torch.zeros(1, batch_size, hidden_size)  # Initial hidden state
c0 = torch.zeros(1, batch_size, hidden_size)  # Initial cell state (LSTM)

# torch.randn() - Random weight initialization
input_size = 100
weights = torch.randn(hidden_size, input_size) * 0.01  # Small random weights

# torch.cat() - Concatenate tensors (used in stacking layers)
tensor1 = torch.randn(32, 50)
tensor2 = torch.randn(32, 50)
combined = torch.cat([tensor1, tensor2], dim=1)  # Shape: (32, 100)

# torch.stack() - Stack tensors (create sequence dimension)
timestep1 = torch.randn(32, 100)
timestep2 = torch.randn(32, 100)
timestep3 = torch.randn(32, 100)
sequence = torch.stack([timestep1, timestep2, timestep3], dim=1)  # Shape: (32, 3, 100)

# .unsqueeze() - Add dimensions for batch/sequence formatting
single_sample = torch.randn(50)
batched = single_sample.unsqueeze(0)  # Add batch dimension: (1, 50)
sequenced = batched.unsqueeze(1)      # Add sequence dimension: (1, 1, 50)

# .squeeze() - Remove single dimensions
reduced = sequenced.squeeze()  # Back to (50,)

# .view() / .reshape() - Reshape tensors
original = torch.randn(32, 10, 100)
flattened = original.view(32, -1)  # Shape: (32, 1000)
reshaped = original.reshape(320, 100)  # Shape: (320, 100)

# Slicing for sequence operations
last_output = sequence[:, -1, :]  # Get last timestep output
first_half = sequence[:, :5, :]   # Get first 5 timesteps
```

### Essential RNN/LSTM Layer Functions

```python
# ====== CORE RNN/LSTM LAYERS ======

# nn.RNN() - Vanilla RNN layer
rnn = nn.RNN(
    input_size=100,      # Input feature size
    hidden_size=128,     # Hidden state size
    num_layers=2,        # Number of stacked layers
    batch_first=True,    # Input shape: (batch, seq, features)
    dropout=0.2,         # Dropout between layers
    bidirectional=False  # Unidirectional
)

# nn.LSTM() - LSTM layer
lstm = nn.LSTM(
    input_size=100,
    hidden_size=128,
    num_layers=2,
    batch_first=True,
    dropout=0.3,
    bidirectional=True   # Bidirectional LSTM
)

# nn.GRU() - GRU layer (simpler than LSTM)
gru = nn.GRU(
    input_size=100,
    hidden_size=128,
    num_layers=1,
    batch_first=True,
    dropout=0.0,
    bidirectional=False
)

# For bidirectional: hidden_size becomes 2*hidden_size in output
print(f"LSTM output size: {128 * 2}")  # 256 for bidirectional
```

### Activation Functions Used in RNNs

```python
# ====== ACTIVATION FUNCTIONS ======

# torch.tanh() - Default RNN activation
x = torch.randn(32, 128)
tanh_output = torch.tanh(x)  # Range: [-1, 1]

# torch.sigmoid() - Used in LSTM gates
sigmoid_output = torch.sigmoid(x)  # Range: [0, 1]

# F.relu() - Can be used in output layers
relu_output = F.relu(x)  # Range: [0, inf)

# F.softmax() - For classification output
logits = torch.randn(32, 10)  # 10 classes
probabilities = F.softmax(logits, dim=1)  # Sum to 1 across classes

# F.log_softmax() - For numerical stability with NLLLoss
log_probs = F.log_softmax(logits, dim=1)
```

## RNN Implementation from Scratch

```python
class VanillaRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(VanillaRNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # Weight matrices
        self.W_ih = nn.Parameter(torch.randn(input_size, hidden_size) * 0.01)   # Input to hidden
        self.W_hh = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.01) # Hidden to hidden
        self.W_ho = nn.Parameter(torch.randn(hidden_size, output_size) * 0.01) # Hidden to output
        
        # Bias vectors
        self.b_h = nn.Parameter(torch.zeros(hidden_size))
        self.b_o = nn.Parameter(torch.zeros(output_size))
        
    def forward(self, x, hidden=None):
        batch_size, seq_len, _ = x.size()
        
        # Initialize hidden state
        if hidden is None:
            hidden = torch.zeros(batch_size, self.hidden_size, device=x.device)
        
        outputs = []
        
        # Process each timestep
        for t in range(seq_len):
            x_t = x[:, t, :]  # Current input
            
            # RNN computation: h_t = tanh(x_t @ W_ih + h_prev @ W_hh + b_h)
            hidden = torch.tanh(x_t @ self.W_ih + hidden @ self.W_hh + self.b_h)
            
            # Output: y_t = h_t @ W_ho + b_o
            output = hidden @ self.W_ho + self.b_o
            outputs.append(output)
        
        # Stack outputs: (batch_size, seq_len, output_size)
        outputs = torch.stack(outputs, dim=1)
        
        return outputs, hidden

# Test the RNN
rnn_model = VanillaRNN(input_size=50, hidden_size=100, output_size=10)
test_input = torch.randn(32, 15, 50)  # (batch=32, seq=15, features=50)
outputs, final_hidden = rnn_model(test_input)
```

## LSTM Implementation from Scratch

```python
class CustomLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(CustomLSTM, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        # LSTM has 4 gates: forget, input, candidate, output
        # Each gate has weights for input and hidden state
        
        # Forget gate: decides what to throw away from cell state
        self.W_f = nn.Linear(input_size + hidden_size, hidden_size)
        
        # Input gate: decides what new information to store
        self.W_i = nn.Linear(input_size + hidden_size, hidden_size)
        
        # Candidate gate: creates new candidate values
        self.W_c = nn.Linear(input_size + hidden_size, hidden_size)
        
        # Output gate: decides what parts of cell state to output
        self.W_o = nn.Linear(input_size + hidden_size, hidden_size)
        
        # Output layer
        self.W_y = nn.Linear(hidden_size, output_size)
        
    def forward(self, x, states=None):
        batch_size, seq_len, _ = x.size()
        
        # Initialize states
        if states is None:
            h_t = torch.zeros(batch_size, self.hidden_size, device=x.device)
            c_t = torch.zeros(batch_size, self.hidden_size, device=x.device)
        else:
            h_t, c_t = states
        
        outputs = []
        
        for t in range(seq_len):
            x_t = x[:, t, :]
            
            # Concatenate input and previous hidden state
            combined = torch.cat([x_t, h_t], dim=1)
            
            # LSTM Gates computation
            f_t = torch.sigmoid(self.W_f(combined))  # Forget gate
            i_t = torch.sigmoid(self.W_i(combined))  # Input gate  
            c_tilde = torch.tanh(self.W_c(combined)) # Candidate values
            o_t = torch.sigmoid(self.W_o(combined))  # Output gate
            
            # Update cell state: C_t = f_t * C_(t-1) + i_t * C_tilde
            c_t = f_t * c_t + i_t * c_tilde
            
            # Update hidden state: h_t = o_t * tanh(C_t)
            h_t = o_t * torch.tanh(c_t)
            
            # Compute output
            y_t = self.W_y(h_t)
            outputs.append(y_t)
        
        outputs = torch.stack(outputs, dim=1)
        return outputs, (h_t, c_t)

# Test LSTM
lstm_model = CustomLSTM(input_size=50, hidden_size=100, output_size=10)
outputs, (final_h, final_c) = lstm_model(test_input)
```

## RNN Architecture Types

```python
# ====== 1. MANY-TO-ONE (Sequence Classification) ======
class ManyToOne(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(ManyToOne, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.classifier = nn.Linear(hidden_size, num_classes)
        
    def forward(self, x):
        # x shape: (batch, seq_len, input_size)
        lstm_out, (h_n, c_n) = self.lstm(x)
        
        # Use last hidden state for classification
        last_hidden = h_n[-1]  # Take last layer's hidden state
        output = self.classifier(last_hidden)
        return output

# ====== 2. ONE-TO-MANY (Sequence Generation) ======
class OneToMany(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, seq_len):
        super(OneToMany, self).__init__()
        self.hidden_size = hidden_size
        self.seq_len = seq_len
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.output_layer = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        # x shape: (batch, 1, input_size) - single input
        batch_size = x.size(0)
        
        # Initialize hidden state
        h0 = torch.zeros(1, batch_size, self.hidden_size, device=x.device)
        c0 = torch.zeros(1, batch_size, self.hidden_size, device=x.device)
        
        outputs = []
        hidden = (h0, c0)
        
        # Generate sequence
        for _ in range(self.seq_len):
            lstm_out, hidden = self.lstm(x, hidden)
            output = self.output_layer(lstm_out)
            outputs.append(output)
            x = output  # Use output as next input
            
        return torch.cat(outputs, dim=1)

# ====== 3. MANY-TO-MANY (Sequence to Sequence) ======
class ManyToMany(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(ManyToMany, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.output_layer = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        # x shape: (batch, seq_len, input_size)
        lstm_out, _ = self.lstm(x)
        # lstm_out shape: (batch, seq_len, hidden_size)
        
        # Apply output layer to each timestep
        output = self.output_layer(lstm_out)
        return output

# ====== 4. ENCODER-DECODER (Different Length Sequences) ======
class EncoderDecoder(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, output_seq_len):
        super(EncoderDecoder, self).__init__()
        self.hidden_size = hidden_size
        self.output_seq_len = output_seq_len
        
        # Encoder
        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)
        
        # Decoder
        self.decoder = nn.LSTM(output_size, hidden_size, batch_first=True)
        self.output_layer = nn.Linear(hidden_size, output_size)
        
    def forward(self, src, tgt=None):
        batch_size = src.size(0)
        
        # Encode
        _, (h_n, c_n) = self.encoder(src)
        
        # Decode
        if self.training and tgt is not None:
            # Teacher forcing during training
            decoder_out, _ = self.decoder(tgt, (h_n, c_n))
            output = self.output_layer(decoder_out)
        else:
            # Inference mode
            outputs = []
            hidden = (h_n, c_n)
            decoder_input = torch.zeros(batch_size, 1, self.output_layer.out_features, device=src.device)
            
            for _ in range(self.output_seq_len):
                decoder_out, hidden = self.decoder(decoder_input, hidden)
                output = self.output_layer(decoder_out)
                outputs.append(output)
                decoder_input = output
                
            output = torch.cat(outputs, dim=1)
            
        return output
```

## LSTM Variants and Advanced Techniques

```python
# ====== BIDIRECTIONAL LSTM ======
class BiLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(BiLSTM, self).__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True,  # KEY: Makes it bidirectional
            dropout=0.2 if num_layers > 1 else 0
        )
        
        # Output size is 2*hidden_size due to bidirectionality
        self.output_layer = nn.Linear(hidden_size * 2, output_size)
        
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        # lstm_out shape: (batch, seq_len, hidden_size * 2)
        output = self.output_layer(lstm_out)
        return output

# ====== STACKED LSTM ======
class StackedLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=3):
        super(StackedLSTM, self).__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,  # Multiple layers
            batch_first=True,
            dropout=0.3  # Dropout between layers
        )
        self.output_layer = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        output = self.output_layer(lstm_out)
        return output

# ====== LSTM WITH ATTENTION ======
class LSTMWithAttention(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(LSTMWithAttention, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        
        # Attention mechanism
        self.attention = nn.Linear(hidden_size, 1)
        self.output_layer = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        # lstm_out shape: (batch, seq_len, hidden_size)
        
        # Compute attention weights
        attention_weights = F.softmax(self.attention(lstm_out), dim=1)
        # attention_weights shape: (batch, seq_len, 1)
        
        # Apply attention weights
        context = torch.sum(attention_weights * lstm_out, dim=1)
        # context shape: (batch, hidden_size)
        
        output = self.output_layer(context)
        return output, attention_weights

# ====== LSTM WITH RESIDUAL CONNECTIONS ======
class ResidualLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=4):
        super(ResidualLSTM, self).__init__()
        self.num_layers = num_layers
        
        # First layer
        self.lstm_layers = nn.ModuleList([
            nn.LSTM(input_size, hidden_size, batch_first=True)
        ])
        
        # Remaining layers
        for _ in range(num_layers - 1):
            self.lstm_layers.append(
                nn.LSTM(hidden_size, hidden_size, batch_first=True)
            )
        
        self.output_layer = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        current_input = x
        
        for i, lstm_layer in enumerate(self.lstm_layers):
            lstm_out, _ = lstm_layer(current_input)
            
            # Add residual connection (skip connection)
            if i > 0 and current_input.size(-1) == lstm_out.size(-1):
                lstm_out = lstm_out + current_input
            
            current_input = lstm_out
            
        output = self.output_layer(lstm_out)
        return output

# ====== CONVOLUTIONAL LSTM (ConvLSTM) ======
class ConvLSTMCell(nn.Module):
    def __init__(self, input_channels, hidden_channels, kernel_size):
        super(ConvLSTMCell, self).__init__()
        self.hidden_channels = hidden_channels
        padding = kernel_size // 2
        
        # Convolutional layers for gates
        self.conv = nn.Conv2d(
            input_channels + hidden_channels,
            4 * hidden_channels,  # 4 gates
            kernel_size,
            padding=padding
        )
        
    def forward(self, x, states):
        h, c = states
        combined = torch.cat([x, h], dim=1)
        gates = self.conv(combined)
        
        # Split into 4 gates
        f_gate, i_gate, c_gate, o_gate = torch.split(gates, self.hidden_channels, dim=1)
        
        f_gate = torch.sigmoid(f_gate)
        i_gate = torch.sigmoid(i_gate)
        c_gate = torch.tanh(c_gate)
        o_gate = torch.sigmoid(o_gate)
        
        c_next = f_gate * c + i_gate * c_gate
        h_next = o_gate * torch.tanh(c_next)
        
        return h_next, c_next
```

## Enhancement Techniques

```python
# ====== DROPOUT TECHNIQUES ======
class LSTMWithDropout(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, dropout_prob=0.5):
        super(LSTMWithDropout, self).__init__()
        
        # Input dropout
        self.input_dropout = nn.Dropout(dropout_prob)
        
        # LSTM with internal dropout
        self.lstm = nn.LSTM(
            input_size, hidden_size, 
            batch_first=True,
            dropout=dropout_prob,  # Between LSTM layers
            num_layers=2
        )
        
        # Output dropout
        self.output_dropout = nn.Dropout(dropout_prob)
        self.output_layer = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        x = self.input_dropout(x)
        lstm_out, _ = self.lstm(x)
        lstm_out = self.output_dropout(lstm_out)
        output = self.output_layer(lstm_out)
        return output

# ====== LAYER NORMALIZATION ======
class LSTMWithLayerNorm(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(LSTMWithLayerNorm, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.layer_norm = nn.LayerNorm(hidden_size)
        self.output_layer = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        normalized_out = self.layer_norm(lstm_out)
        output = self.output_layer(normalized_out)
        return output

# ====== GRADIENT CLIPPING ======
def train_with_gradient_clipping(model, dataloader, optimizer, criterion, max_norm=1.0):
    model.train()
    total_loss = 0
    
    for batch_x, batch_y in dataloader:
        optimizer.zero_grad()
        
        # Forward pass
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        
        # Backward pass
        loss.backward()
        
        # Gradient clipping - CRUCIAL for RNN/LSTM training
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)
        
        optimizer.step()
        total_loss += loss.item()
    
    return total_loss / len(dataloader)

# ====== LEARNING RATE SCHEDULING ======
def get_scheduler(optimizer, scheduler_type='cosine'):
    if scheduler_type == 'cosine':
        return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)
    elif scheduler_type == 'step':
        return optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
    elif scheduler_type == 'plateau':
        return optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)
    else:
        return None
```

## Practical Implementation Examples

```python
# ====== SENTIMENT ANALYSIS EXAMPLE ======
class SentimentAnalyzer(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes=2):
        super(SentimentAnalyzer, self).__init__()
        
        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # Bidirectional LSTM
        self.lstm = nn.LSTM(
            embed_dim, hidden_size,
            batch_first=True,
            bidirectional=True,
            dropout=0.3,
            num_layers=2
        )
        
        # Attention mechanism
        self.attention = nn.Linear(hidden_size * 2, 1)
        
        # Classification head
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size, num_classes)
        )
        
    def forward(self, x):
        # x shape: (batch, seq_len)
        embedded = self.embedding(x)
        
        lstm_out, _ = self.lstm(embedded)
        
        # Apply attention
        attention_weights = F.softmax(self.attention(lstm_out), dim=1)
        context = torch.sum(attention_weights * lstm_out, dim=1)
        
        output = self.classifier(context)
        return output

# ====== TIME SERIES PREDICTION ======
class TimeSeriesPredictor(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers=2, forecast_horizon=1):
        super(TimeSeriesPredictor, self).__init__()
        
        # Multi-layer LSTM
        self.lstm = nn.LSTM(
            input_size, hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.2
        )
        
        # Prediction head
        self.predictor = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size // 2, forecast_horizon)
        )
        
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        
        # Use last timestep for prediction
        last_output = lstm_out[:, -1, :]
        prediction = self.predictor(last_output)
        
        return prediction

# ====== SEQUENCE GENERATION ======
class SequenceGenerator(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers=2):
        super(SequenceGenerator, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, batch_first=True)
        self.output_layer = nn.Linear(hidden_size, vocab_size)
        
    def forward(self, x, hidden=None):
        embedded = self.embedding(x)
        lstm_out, hidden = self.lstm(embedded, hidden)
        output = self.output_layer(lstm_out)
        return output, hidden
    
    def generate(self, start_token, max_length=50, temperature=1.0):
        self.eval()
        with torch.no_grad():
            generated = [start_token]
            hidden = None
            
            for _ in range(max_length):
                input_token = torch.tensor([[generated[-1]]])
                output, hidden = self.forward(input_token, hidden)
                
                # Apply temperature
                logits = output[0, -1, :] / temperature
                probs = F.softmax(logits, dim=0)
                
                # Sample next token
                next_token = torch.multinomial(probs, 1).item()
                generated.append(next_token)
                
                # Stop if end token
                if next_token == 0:  # Assuming 0 is end token
                    break
                    
        return generated
```

## Optimization and Training Tips

```python
# ====== ADVANCED TRAINING LOOP ======
def train_rnn_model(model, train_loader, val_loader, num_epochs=100):
    # Optimizer with weight decay
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    
    # Learning rate scheduler
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer, max_lr=0.01, epochs=num_epochs, 
        steps_per_epoch=len(train_loader)
    )
    
    # Loss function
    criterion = nn.CrossEntropyLoss()
    
    # Early stopping
    best_val_loss = float('inf')
    patience = 10
    patience_counter = 0
    
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0
        
        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            scheduler.step()
            
            train_loss += loss.item()
        
        # Validation phase
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item()
        
        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        
        # Early stopping check
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            # Save best model
            torch.save(model.state_dict(), 'best_model.pth')
        else:
            patience_counter += 1
            
        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch}")
            break
            
        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")

# ====== HYPERPARAMETER TUNING ======
def hyperparameter_search():
    # Define search space
    hyperparams = {
        'hidden_size': [64, 128, 256, 512],
        '
